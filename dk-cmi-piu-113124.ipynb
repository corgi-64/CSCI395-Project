{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Based on:\n- https://www.kaggle.com/code/mgmostafa/cmi-best-single-model-simplified/output\n- https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet","metadata":{}},{"cell_type":"markdown","source":"# Notes on Successful approaches:\n- The best performing public model uses Ensemble learning, specifically a **Voting Regressor**, which combines predictions from LightGBM, XGBoost and CatBoost. \"This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.\"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom typing import List\n\nfrom scipy import stats\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import clone\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\n# from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n# from sklearn.impute import SimpleImputer, KNNImputer\n# from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, cohen_kappa_score\nfrom scipy.stats import randint, uniform\nfrom sklearn.feature_selection import mutual_info_classif\nfrom scipy.optimize import minimize\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n# from lightgbm import LGBMClassifier\n# from xgboost import XGBClassifier\n# from catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:21.274633Z","iopub.execute_input":"2024-12-01T18:23:21.274980Z","iopub.status.idle":"2024-12-01T18:23:21.282458Z","shell.execute_reply.started":"2024-12-01T18:23:21.274947Z","shell.execute_reply":"2024-12-01T18:23:21.281539Z"}},"outputs":[],"execution_count":328},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_pred_rounded = np.round(y_pred).clip(0, 3).astype(int)\n    return cohen_kappa_score(y_true, y_pred_rounded, weights='quadratic')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:21.547153Z","iopub.execute_input":"2024-12-01T18:23:21.547438Z","iopub.status.idle":"2024-12-01T18:23:21.551598Z","shell.execute_reply.started":"2024-12-01T18:23:21.547413Z","shell.execute_reply":"2024-12-01T18:23:21.550678Z"}},"outputs":[],"execution_count":329},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    \n    # Calculate time gaps\n    df['time'] = pd.to_datetime(df['time_of_day'])\n    df['time_gap'] = df['time'].diff().dt.total_seconds()\n    \n    # Define activity/inactivity\n    enmo_threshold = 0.003\n    non_wear = df['non-wear_flag'] == 1\n    low_enmo = df['enmo'] < enmo_threshold\n    large_gap = df['time_gap'] > 5\n\n    is_inactive = non_wear | low_enmo | large_gap\n    # print(f\"Total inactive records: {is_inactive.sum()} ({is_inactive.mean()*100:.2f}%)\")\n    # print(f\"Total active records: {(~is_inactive).sum()} ({(~is_inactive).mean()*100:.2f}%)\")\n\n    # print(f\"\\nDebugging file {filename}:\")\n    # print(f\"Total records: {len(df)}\")\n    # print(f\"Records with non-wear flag: {non_wear.sum()} ({non_wear.mean()*100:.2f}%)\")\n    # print(f\"Records with low ENMO: {low_enmo.sum()} ({low_enmo.mean()*100:.2f}%)\")\n    # print(f\"Records with large gaps: {large_gap.sum()} ({large_gap.mean()*100:.2f}%)\")\n\n    active_df = df[~is_inactive].copy()  # Get only active periods\n    \n    df['hour'] = df['time'].dt.hour\n    active_df['hour'] = active_df['time'].dt.hour\n    \n    # Basic stats for all periods\n    all_basic_stats = df[['enmo', 'anglez', 'X', 'Y', 'Z', 'light', 'battery_voltage', 'time_gap']].describe().values.reshape(-1)\n    \n    # Basic stats for active periods only\n    active_basic_stats = active_df[['enmo', 'anglez', 'X', 'Y', 'Z', 'light', 'battery_voltage', 'time_gap']].describe().values.reshape(-1)\n\n\n    daytime_enmo = df[df['hour'].between(9, 17)]['enmo'].mean() or 0   # removed comma\n    nighttime_enmo = df[~df['hour'].between(9, 17)]['enmo'].mean() or 0\n    active_daytime = active_df[active_df['hour'].between(9, 17)]['enmo'].mean() or 0   # removed comma\n    active_nighttime = active_df[~active_df['hour'].between(9, 17)]['enmo'].mean() or 0\n\n    \n    # Additional features for all periods\n    all_key_features = [\n        (~is_inactive).mean(),  # Activity ratio\n        daytime_enmo,\n        nighttime_enmo,\n        # df[df['hour'].between(9, 17)]['enmo'].mean(),  # Daytime activity\n        # df[~df['hour'].between(9, 17)]['enmo'].mean(),  # Non-daytime activity\n        df['time_gap'].gt(5).mean()  # Proportion of large gaps\n    ]\n    \n    # Additional features for active periods\n    active_key_features = [\n        active_daytime,\n        active_nighttime,\n        # active_df[active_df['hour'].between(9, 17)]['enmo'].mean(),  # Active daytime activity\n        # active_df[~active_df['hour'].between(9, 17)]['enmo'].mean(),  # Active non-daytime activity\n        active_df['time_gap'].gt(5).mean()  # Proportion of large gaps in active periods\n    ]\n    \n    # print(\"\\nFeature stats:\")\n    # print(\"All basic stats shape:\", all_basic_stats.shape)\n    # print(\"Active basic stats shape:\", active_basic_stats.shape)\n    # print(\"All key features:\", all_key_features)\n    # print(\"Active key features:\", active_key_features)\n    \n    features = np.concatenate([all_basic_stats, all_key_features, active_basic_stats, active_key_features])\n    # print(\"Final features shape:\", features.shape)\n    # print(\"Any NaN in features:\", np.isnan(features).any())\n    \n    return features, filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n\n    # test_ids = ids[:5] # temp\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(\n            executor.map(lambda fname: process_file(fname, dirname), ids),\n            total=len(ids))\n            # executor.map(lambda fname: process_file(fname, dirname), test_ids),\n            # total=len(test_ids))\n            \n        )\n    \n    stats, indexes = zip(*results)\n    \n    # Column names\n    cols = []\n    # For all periods\n    for var in ['enmo', 'anglez', 'X', 'Y', 'Z', 'light', 'battery_voltage', 'time_gap']:\n        for stat in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n            cols.append(f'all_{var}_{stat}')\n    \n    cols.extend(['activity_ratio', 'all_daytime_activity', 'all_nighttime_activity', 'all_gap_ratio'])\n    \n    # For active periods\n    for var in ['enmo', 'anglez', 'X', 'Y', 'Z', 'light', 'battery_voltage', 'time_gap']:\n        for stat in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n            cols.append(f'active_{var}_{stat}')\n    \n    cols.extend(['active_daytime_activity', 'active_nighttime_activity', 'active_gap_ratio'])\n    \n    df = pd.DataFrame(stats, columns=cols)\n    df['id'] = indexes\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:21.822432Z","iopub.execute_input":"2024-12-01T18:23:21.822662Z","iopub.status.idle":"2024-12-01T18:23:21.835482Z","shell.execute_reply.started":"2024-12-01T18:23:21.822639Z","shell.execute_reply":"2024-12-01T18:23:21.834616Z"}},"outputs":[],"execution_count":330},{"cell_type":"code","source":"# # MOST BASIC PROCESSING OF PARQUET (JUST GETS SUMMARY STATS FOR EACH PARQUET FEATURE)\n\n# def process_file(filename, dirname):\n#     df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n#     df.drop('step', axis=1, inplace=True)\n#     return df.describe().values.reshape(-1), filename.split('=')[1]\n\n# def load_time_series(dirname) -> pd.DataFrame:\n#     ids = os.listdir(dirname)\n    \n#     with ThreadPoolExecutor() as executor:\n#         results = list(tqdm(\n#             executor.map(lambda fname: process_file(fname, dirname), ids),\n#             total=len(ids))\n#         )\n    \n#     stats, indexes = zip(*results)\n    \n#     # Create generic column names for all statistics\n#     df = pd.DataFrame(stats, columns=[f\"Stat_{i}\" for i in range(len(stats[0]))])\n#     df['id'] = indexes\n    \n#     return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:22.100547Z","iopub.execute_input":"2024-12-01T18:23:22.101361Z","iopub.status.idle":"2024-12-01T18:23:22.105758Z","shell.execute_reply.started":"2024-12-01T18:23:22.101319Z","shell.execute_reply":"2024-12-01T18:23:22.104838Z"}},"outputs":[],"execution_count":331},{"cell_type":"code","source":"def preprocess_data(df: pd.DataFrame, is_train: bool = False, encoders: dict = None, return_encoders: bool = False) -> pd.DataFrame:\n    \"\"\"Preprocess data including encoding and imputation\"\"\"\n    df = df.copy()\n    \n    # First handle missing values\n    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    for col in numerical_cols:\n        df[col] = df[col].fillna(df[col].median())\n        \n    # Identify categorical columns (including seasons)\n    categorical_cols = df.select_dtypes(include=['object']).columns\n    \n    # Handle missing categoricals\n    for col in categorical_cols:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    \n    # Encode categorical variables\n    if is_train:\n        # For training data, create new encoders\n        encoders = {}\n        for col in categorical_cols:\n            if col != 'id':  # Skip ID column\n                encoders[col] = LabelEncoder()\n                df[col] = encoders[col].fit_transform(df[col])\n        # Decide whether to return encoders\n        return (df, encoders) if return_encoders else df\n    else:\n        if encoders is None:\n            raise ValueError(\"Must provide encoders when preprocessing test data\")\n        # For test data, use provided encoders\n        for col in categorical_cols:\n            if col != 'id' and col in encoders:  # Skip ID and handle only columns present in train\n                df[col] = encoders[col].transform(df[col])\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:22.429953Z","iopub.execute_input":"2024-12-01T18:23:22.430197Z","iopub.status.idle":"2024-12-01T18:23:22.437259Z","shell.execute_reply.started":"2024-12-01T18:23:22.430173Z","shell.execute_reply":"2024-12-01T18:23:22.436511Z"}},"outputs":[],"execution_count":332},{"cell_type":"code","source":"def process_features(main_df: pd.DataFrame, parquet_features: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Process all features in one go\"\"\"\n    # If supervised_df is a tuple (from preprocess_data), take just the DataFrame\n    if isinstance(main_df, tuple):\n        main_df = main_df[0] # don't need encoders object\n    \n    # OPTIONAL: GENERATE MORE FEATURES (Removed for now)\n    # feature_generator = ActivityFeatureGenerator()\n    # df_features = feature_generator.generate_features(parquet_features)\n    \n    # Join with original data\n    df_combined = pd.merge(\n        main_df,\n        # df_features,\n        parquet_features,\n        on='id',\n        how='left'\n    )\n    \n    return df_combined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:22.686442Z","iopub.execute_input":"2024-12-01T18:23:22.687105Z","iopub.status.idle":"2024-12-01T18:23:22.691293Z","shell.execute_reply.started":"2024-12-01T18:23:22.687079Z","shell.execute_reply":"2024-12-01T18:23:22.690471Z"}},"outputs":[],"execution_count":333},{"cell_type":"code","source":"def find_correlated_features(X, threshold=0.95):\n    \"\"\"Identifies correlated features to remove based on training data\"\"\"\n    corr_matrix = X.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    # Find features to drop\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print(f\"Identified {len(to_drop)} correlated features to remove\")\n    return to_drop\n\ndef remove_correlated_features(X, features_to_drop):\n    \"\"\"Removes specified features from a dataset\"\"\"\n    X_reduced = X.drop(columns=features_to_drop)\n    return X_reduced","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:22.978399Z","iopub.execute_input":"2024-12-01T18:23:22.978654Z","iopub.status.idle":"2024-12-01T18:23:22.983998Z","shell.execute_reply.started":"2024-12-01T18:23:22.978629Z","shell.execute_reply":"2024-12-01T18:23:22.983109Z"}},"outputs":[],"execution_count":334},{"cell_type":"markdown","source":"# Make a quick model","metadata":{}},{"cell_type":"code","source":"class_weights = {\n    0: 1.0,\n    1: 2.2,  \n    2: 4.2,  \n    3: 48.0  \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:23.665438Z","iopub.execute_input":"2024-12-01T18:23:23.666355Z","iopub.status.idle":"2024-12-01T18:23:23.670483Z","shell.execute_reply.started":"2024-12-01T18:23:23.666295Z","shell.execute_reply":"2024-12-01T18:23:23.669530Z"}},"outputs":[],"execution_count":335},{"cell_type":"code","source":"%%time\n# Functions for training the evaluating the selected model \ndef threshold_Rounder(oof_non_rounded, thresholds):\n    buffer2 = 0.2  # Buffer for class 2\n    buffer3 = 0.3  # Larger buffer for class 3\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                   np.where(oof_non_rounded < thresholds[1], 1,\n                           np.where(oof_non_rounded < thresholds[2] - buffer2, 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:24.079964Z","iopub.execute_input":"2024-12-01T18:23:24.080437Z","iopub.status.idle":"2024-12-01T18:23:24.087963Z","shell.execute_reply.started":"2024-12-01T18:23:24.080393Z","shell.execute_reply":"2024-12-01T18:23:24.087003Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 12 µs, sys: 0 ns, total: 12 µs\nWall time: 17.4 µs\n","output_type":"stream"}],"execution_count":336},{"cell_type":"code","source":"import torch\n\ndef get_device_params():\n    # if os.environ.get('KAGGLE_IS_KAGGLE_GPU_ENABLED', '') == 'True':\n    if torch.cuda.is_available():\n        XGB_device = {'tree_method': 'gpu_hist'}\n        CB_device = {'task_type': 'GPU'}\n        print(\"GPU detected - using GPU accelerated training\")\n    else:\n        XGB_device = {'tree_method': 'hist'}\n        CB_device = {'task_type': 'CPU'}\n        print(\"No GPU detected - using CPU training\")\n    return XGB_device, CB_device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:24.657545Z","iopub.execute_input":"2024-12-01T18:23:24.657828Z","iopub.status.idle":"2024-12-01T18:23:24.662795Z","shell.execute_reply.started":"2024-12-01T18:23:24.657801Z","shell.execute_reply":"2024-12-01T18:23:24.661762Z"}},"outputs":[],"execution_count":337},{"cell_type":"code","source":"xgb_device, cb_device = get_device_params()\n\nSEED = 42\nn_splits = 5\n\nLGB_Params = {\n    'learning_rate': 0.07, \n    'random_state': SEED, \n    'n_estimators': 200,\n    \n    'max_depth': 8, \n    'num_leaves': 300, \n    'min_data_in_leaf': 17,\n    # 'max_depth': 6,  # Reduced from 8\n    # 'min_data_in_leaf': 10,  # Reduced from 17\n    # 'num_leaves': 150,  # Reduced from 300\n    \n    'feature_fraction': 0.7689, \n    'bagging_fraction': 0.6879, \n    'bagging_freq': 2, \n    'lambda_l1': 4.74, \n    'lambda_l2': 4.743e-06,\n    'verbose': -1,\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED,\n    **xgb_device  # This will add either GPU or CPU parameters\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10,\n    **cb_device  # This will add either GPU or CPU parameters\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:25.210660Z","iopub.execute_input":"2024-12-01T18:23:25.210949Z","iopub.status.idle":"2024-12-01T18:23:25.217555Z","shell.execute_reply.started":"2024-12-01T18:23:25.210921Z","shell.execute_reply":"2024-12-01T18:23:25.216588Z"}},"outputs":[{"name":"stdout","text":"GPU detected - using GPU accelerated training\n","output_type":"stream"}],"execution_count":338},{"cell_type":"markdown","source":"# Try ensembling","metadata":{}},{"cell_type":"code","source":"models = {\n    'lgbm': LGBMRegressor(**LGB_Params),\n    'xgboost': XGBRegressor(**XGB_Params),\n    'catboost': CatBoostRegressor(**CatBoost_Params)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:28.602841Z","iopub.execute_input":"2024-12-01T18:23:28.603677Z","iopub.status.idle":"2024-12-01T18:23:28.607827Z","shell.execute_reply.started":"2024-12-01T18:23:28.603639Z","shell.execute_reply":"2024-12-01T18:23:28.606986Z"}},"outputs":[],"execution_count":339},{"cell_type":"code","source":"def analyze_predictions_distribution(y_true, predictions, name=\"\"):\n    print(f\"\\n{'-'*20} {name} Prediction Analysis {'-'*20}\")\n    \n    # Get distribution of predictions\n    pred_dist = pd.Series(predictions).value_counts(normalize=True).sort_index() * 100\n    \n    if y_true is not None:\n        # Get distribution of true values\n        true_dist = pd.Series(y_true).value_counts(normalize=True).sort_index() * 100\n        \n        print(\"\\nClass Distribution Comparison:\")\n        comparison_df = pd.DataFrame({\n            'True %': true_dist,\n            'Predicted %': pred_dist\n        }).round(2)\n        print(comparison_df)\n        \n        # Confusion matrix for detailed analysis\n        conf_matrix = pd.crosstab(y_true, predictions, \n                                normalize=True) * 100  # Changed from 'true' to True\n        print(\"\\nConfusion Matrix (% of true classes predicted as each class):\")\n        print(conf_matrix.round(2))\n        \n        # Analysis of severe cases (class 3)\n        if 3 in y_true:\n            severe_mask = y_true == 3\n            severe_predictions = predictions[severe_mask]\n            print(f\"\\nDetailed Analysis of Severe Cases (Class 3):\")\n            print(f\"Total Severe Cases: {sum(severe_mask)}\")\n            print(\"Predicted as:\")\n            for pred_class in range(4):\n                count = sum(severe_predictions == pred_class)\n                percent = (count / sum(severe_mask)) * 100\n                print(f\"Class {pred_class}: {count} cases ({percent:.2f}%)\")\n    else:\n        print(\"\\nPrediction Distribution:\")\n        for class_val, percentage in pred_dist.items():\n            print(f\"Class {class_val}: {percentage:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:29.234416Z","iopub.execute_input":"2024-12-01T18:23:29.235274Z","iopub.status.idle":"2024-12-01T18:23:29.242714Z","shell.execute_reply.started":"2024-12-01T18:23:29.235202Z","shell.execute_reply":"2024-12-01T18:23:29.241882Z"}},"outputs":[],"execution_count":340},{"cell_type":"code","source":"def TrainML(model_class, test_data):\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n    \n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        \n        # Create sample weights for all models\n        sample_weights = np.array([class_weights[yi] for yi in y_train])\n        \n        # All models use sample_weight\n        model.fit(X_train, y_train, sample_weight=sample_weights)\n        \n        # model = clone(model_class)\n        # model.fit(X_train, y_train)\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n    \n    analyze_predictions_distribution(y, oof_rounded, \"OOF\")\n    \n    mean_val_score = np.mean(test_S)\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {mean_val_score:.4f}\")\n    \n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n    \n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': test_id,\n        'sii': tpTuned\n    })\n    \n    return submission, model, tKappa\n\ndef weighted_ensemble_train(X, y, test_data, models_dict):\n    predictions = {}\n    val_scores = {}\n    \n    for name, model in models_dict.items():\n        print(f\"\\nTraining {name}...\")\n        submission, trained_model, val_score = TrainML(model, test_data)\n        predictions[name] = submission['sii']\n        val_scores[name] = val_score\n        print(f\"{name} completed with score: {val_score:.4f}\")\n    \n    # Manual weights favoring CatBoost\n    weights = {\n        'lgbm': 0.25,\n        'xgboost': 0.25,\n        'catboost': 0.50 \n    }\n    \n    print(\"\\nModel Weights:\")\n    for name, weight in weights.items():\n        print(f\"{name}: {weight:.3f}\")\n    \n    # Weighted average\n    ensemble_pred = np.zeros_like(list(predictions.values())[0], dtype=float)\n    for name, pred in predictions.items():\n        ensemble_pred += weights[name] * pred\n\n    final_pred = threshold_Rounder(ensemble_pred, [0.5, 1.5, 2.5])\n    analyze_predictions_distribution(None, final_pred, \"Ensemble Test\")\n        \n    final_submission = pd.DataFrame({\n        'id': test_id,\n        'sii': ensemble_pred.round().astype(int)\n    })\n    \n    return final_submission, val_scores, weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:29.657693Z","iopub.execute_input":"2024-12-01T18:23:29.658371Z","iopub.status.idle":"2024-12-01T18:23:29.671369Z","shell.execute_reply.started":"2024-12-01T18:23:29.658337Z","shell.execute_reply":"2024-12-01T18:23:29.670470Z"}},"outputs":[],"execution_count":341},{"cell_type":"markdown","source":"# Steps for improvement:\n- Address class imbalance\n    - Modify your StratifiedKFold implementation to ensure the rare class (severity 3) is properly represented in each fold:\n    - Add sample weights to give more importance to minority classes:\n    - Modify your threshold optimization to be more sensitive to minority classes:\n    - Adjust model parameters for imbalanced data:\n\n- Add more new features (look at what other contestants are doing)\n- Modify StatifiedKFold to use group-based splitting \n- Consider making model weights adaptive based on validation performance\n- Make threshold rounder more robust\n- Correlation threshold of 0.95 might be too high","metadata":{}},{"cell_type":"code","source":"# Load data\nprint(\"Loading data...\")\nbase_path = '/kaggle/input/child-mind-institute-problematic-internet-use'\ntrain_parquet_path = f'{base_path}/series_train.parquet'\ntest_parquet_path = f'{base_path}/series_test.parquet'\n\n# Load data\ntrain = pd.read_csv(f'{base_path}/train.csv')\ntest = pd.read_csv(f'{base_path}/test.csv')\n\n# Get supervised data (rows with non-null target)\ntrain = train[train['sii'].notna()].copy()\nprint(f\"Train shape: {train.shape}, Test shape: {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:30.573952Z","iopub.execute_input":"2024-12-01T18:23:30.574790Z","iopub.status.idle":"2024-12-01T18:23:30.617791Z","shell.execute_reply.started":"2024-12-01T18:23:30.574753Z","shell.execute_reply":"2024-12-01T18:23:30.616623Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nTrain shape: (2736, 82), Test shape: (20, 59)\n","output_type":"stream"}],"execution_count":342},{"cell_type":"code","source":"# # !ls\n# print(os.getcwd())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:36.505866Z","iopub.execute_input":"2024-12-01T18:23:36.506684Z","iopub.status.idle":"2024-12-01T18:23:36.510214Z","shell.execute_reply.started":"2024-12-01T18:23:36.506646Z","shell.execute_reply":"2024-12-01T18:23:36.509357Z"}},"outputs":[],"execution_count":343},{"cell_type":"code","source":"# # Load parquet features with error handling\n# print(\"\\nProcessing parquet files...\")\n# try:\n#     print(\"Loading and processing training data...\")\n#     train_ts = load_time_series(train_parquet_path)\n#     print(\"\\nShape of training features:\", train_ts.shape)\n    \n#     print(\"\\nLoading and processing test data...\")\n#     test_ts = load_time_series(test_parquet_path)\n#     print(\"\\nShape of test features:\", test_ts.shape)\n    \n#     # Save features\n#     print(\"\\nSaving features...\")\n#     train_ts.to_parquet(\"actigraphy_features_train.parquet\")\n#     test_ts.to_parquet(\"actigraphy_features_test.parquet\")\n# except Exception as e:\n#     print(f\"Error processing parquet files: {str(e)}\")\n#     raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:36.779339Z","iopub.execute_input":"2024-12-01T18:23:36.779565Z","iopub.status.idle":"2024-12-01T18:23:36.783554Z","shell.execute_reply.started":"2024-12-01T18:23:36.779541Z","shell.execute_reply":"2024-12-01T18:23:36.782651Z"}},"outputs":[],"execution_count":344},{"cell_type":"code","source":"# Merge with original data\nprint(\"\\nMerging parquet data with CSV data...\")\ntry:\n    train_ts = pd.read_parquet(\"actigraphy_features_train.parquet\")\n    test_ts = pd.read_parquet(\"actigraphy_features_test.parquet\")\n    \n    train_combined = process_features(train, train_ts)\n    test_combined = process_features(test, test_ts)\n    print(\"\\nInitial shapes:\")\n    print(\"Train combined:\", train_combined.shape)\n    print(\"Test combined:\", test_combined.shape)\nexcept Exception as e:\n    print(f\"Error in merging: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:37.153804Z","iopub.execute_input":"2024-12-01T18:23:37.154513Z","iopub.status.idle":"2024-12-01T18:23:37.190202Z","shell.execute_reply.started":"2024-12-01T18:23:37.154484Z","shell.execute_reply":"2024-12-01T18:23:37.189386Z"}},"outputs":[{"name":"stdout","text":"\nMerging parquet data with CSV data...\n\nInitial shapes:\nTrain combined: (2736, 217)\nTest combined: (20, 194)\n","output_type":"stream"}],"execution_count":345},{"cell_type":"code","source":"# Preprocess with error handling\nprint(\"\\nPreprocessing data...\")\ntry:\n    train_combined, encoders = preprocess_data(train_combined, is_train=True, return_encoders=True)\n    test_combined = preprocess_data(test_combined, is_train=False, encoders=encoders)\nexcept Exception as e:\n    print(f\"Error in preprocessing: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:37.925010Z","iopub.execute_input":"2024-12-01T18:23:37.925600Z","iopub.status.idle":"2024-12-01T18:23:38.111667Z","shell.execute_reply.started":"2024-12-01T18:23:37.925567Z","shell.execute_reply":"2024-12-01T18:23:38.110784Z"}},"outputs":[{"name":"stdout","text":"\nPreprocessing data...\n","output_type":"stream"}],"execution_count":346},{"cell_type":"code","source":"print(\"\\nLast Prep for Training...\")\ntry:\n    exclude_patterns = ['id', 'PCIAT', 'sii']\n    feature_cols = [col for col in train_combined.columns \n                   if not any(pattern in col for pattern in exclude_patterns)]\n    X = train_combined[feature_cols]\n    y = train_combined['sii']\n    features_to_drop = find_correlated_features(X, threshold=0.95) \n    X = remove_correlated_features(X, features_to_drop)\n    \n    test_id = test_combined['id'].copy()\n    test_features = test_combined[feature_cols]\n    test_features = remove_correlated_features(test_features, features_to_drop)\nexcept Exception as e:\n    print()\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:38.252246Z","iopub.execute_input":"2024-12-01T18:23:38.252517Z","iopub.status.idle":"2024-12-01T18:23:38.524379Z","shell.execute_reply.started":"2024-12-01T18:23:38.252490Z","shell.execute_reply":"2024-12-01T18:23:38.523560Z"}},"outputs":[{"name":"stdout","text":"\nLast Prep for Training...\nIdentified 50 correlated features to remove\n","output_type":"stream"}],"execution_count":347},{"cell_type":"code","source":"# Train models with memory efficient processing\nprint(\"\\nTraining models...\")\ntry:\n    models = {\n        'lgbm': LGBMRegressor(**LGB_Params),\n        'xgboost': XGBRegressor(**XGB_Params),\n        'catboost': CatBoostRegressor(**CatBoost_Params)\n    }\n    ensemble_submission, scores, weights = weighted_ensemble_train(X, y, test_features, models)\nexcept Exception as e:\n    print(f\"Error in model training: {str(e)}\")\n    raise\n\n# Save submission\nprint(\"\\nSaving submission...\")\nensemble_submission.to_csv('submission.csv', index=False)\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T18:23:40.266539Z","iopub.execute_input":"2024-12-01T18:23:40.266874Z","iopub.status.idle":"2024-12-01T18:23:57.280899Z","shell.execute_reply.started":"2024-12-01T18:23:40.266843Z","shell.execute_reply":"2024-12-01T18:23:57.279952Z"}},"outputs":[{"name":"stdout","text":"\nTraining models...\n\nTraining lgbm...\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  20%|██        | 1/5 [00:00<00:02,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Train QWK: 0.8556, Validation QWK: 0.3835\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  40%|████      | 2/5 [00:01<00:01,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Train QWK: 0.8488, Validation QWK: 0.3945\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  60%|██████    | 3/5 [00:01<00:01,  1.71it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Train QWK: 0.8528, Validation QWK: 0.3732\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  80%|████████  | 4/5 [00:02<00:00,  1.70it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Train QWK: 0.8618, Validation QWK: 0.3119\n","output_type":"stream"},{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:02<00:00,  1.69it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Train QWK: 0.8561, Validation QWK: 0.3730\n\n-------------------- OOF Prediction Analysis --------------------\n\nClass Distribution Comparison:\n     True %  Predicted %\n0.0   58.26        38.05\n1.0   26.68        54.90\n2.0   13.82         6.94\n3.0    1.24         0.11\n\nConfusion Matrix (% of true classes predicted as each class):\ncol_0      0      1     2     3\nsii                            \n0.0    30.04  26.83  1.39  0.00\n1.0     6.43  17.54  2.63  0.07\n2.0     1.54   9.98  2.27  0.04\n3.0     0.04   0.55  0.66  0.00\n\nDetailed Analysis of Severe Cases (Class 3):\nTotal Severe Cases: 34\nPredicted as:\nClass 0: 1 cases (2.94%)\nClass 1: 15 cases (44.12%)\nClass 2: 18 cases (52.94%)\nClass 3: 0 cases (0.00%)\nMean Train QWK --> 0.8550\nMean Validation QWK ---> 0.3672\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.420\u001b[0m\nlgbm completed with score: 0.4200\n\nTraining xgboost...\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  20%|██        | 1/5 [00:00<00:03,  1.16it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Train QWK: 0.8485, Validation QWK: 0.3801\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  40%|████      | 2/5 [00:01<00:02,  1.29it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Train QWK: 0.8400, Validation QWK: 0.4273\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  60%|██████    | 3/5 [00:02<00:01,  1.32it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Train QWK: 0.8471, Validation QWK: 0.4258\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  80%|████████  | 4/5 [00:03<00:00,  1.35it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Train QWK: 0.8493, Validation QWK: 0.3305\n","output_type":"stream"},{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:03<00:00,  1.34it/s]","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Train QWK: 0.8566, Validation QWK: 0.4069\n\n-------------------- OOF Prediction Analysis --------------------\n\nClass Distribution Comparison:\n     True %  Predicted %\n0.0   58.26        36.73\n1.0   26.68        55.85\n2.0   13.82         7.31\n3.0    1.24         0.11\n\nConfusion Matrix (% of true classes predicted as each class):\ncol_0      0      1     2     3\nsii                            \n0.0    29.86  26.97  1.43  0.00\n1.0     5.74  18.24  2.70  0.00\n2.0     1.13  10.05  2.56  0.07\n3.0     0.00   0.58  0.62  0.04\n\nDetailed Analysis of Severe Cases (Class 3):\nTotal Severe Cases: 34\nPredicted as:\nClass 0: 0 cases (0.00%)\nClass 1: 16 cases (47.06%)\nClass 2: 17 cases (50.00%)\nClass 3: 1 cases (2.94%)\nMean Train QWK --> 0.8483\nMean Validation QWK ---> 0.3941\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.444\u001b[0m\nxgboost completed with score: 0.4442\n\nTraining catboost...\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  20%|██        | 1/5 [00:01<00:06,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 1 - Train QWK: 0.5418, Validation QWK: 0.3614\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  40%|████      | 2/5 [00:03<00:05,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 2 - Train QWK: 0.5344, Validation QWK: 0.3641\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  60%|██████    | 3/5 [00:05<00:03,  1.83s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 3 - Train QWK: 0.5351, Validation QWK: 0.3670\n","output_type":"stream"},{"name":"stderr","text":"Training Folds:  80%|████████  | 4/5 [00:07<00:01,  1.85s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 4 - Train QWK: 0.5549, Validation QWK: 0.3389\n","output_type":"stream"},{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:09<00:00,  1.86s/it]","output_type":"stream"},{"name":"stdout","text":"Fold 5 - Train QWK: 0.5642, Validation QWK: 0.3896\n\n-------------------- OOF Prediction Analysis --------------------\n\nClass Distribution Comparison:\n     True %  Predicted %\n0.0   58.26        24.56\n1.0   26.68        63.27\n2.0   13.82        12.10\n3.0    1.24         0.07\n\nConfusion Matrix (% of true classes predicted as each class):\ncol_0      0      1     2     3\nsii                            \n0.0    21.38  34.14  2.74  0.00\n1.0     2.56  19.85  4.24  0.04\n2.0     0.62   8.88  4.28  0.04\n3.0     0.00   0.40  0.84  0.00\n\nDetailed Analysis of Severe Cases (Class 3):\nTotal Severe Cases: 34\nPredicted as:\nClass 0: 0 cases (0.00%)\nClass 1: 11 cases (32.35%)\nClass 2: 23 cases (67.65%)\nClass 3: 0 cases (0.00%)\nMean Train QWK --> 0.5461\nMean Validation QWK ---> 0.3642\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.445\u001b[0m\ncatboost completed with score: 0.4445\n\nModel Weights:\nlgbm: 0.250\nxgboost: 0.250\ncatboost: 0.500\n\n-------------------- Ensemble Test Prediction Analysis --------------------\n\nPrediction Distribution:\nClass 0: 20.00%\nClass 1: 80.00%\n\nSaving submission...\nDone!\n","output_type":"stream"}],"execution_count":348},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}