{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Based on:\n- https://www.kaggle.com/code/mgmostafa/cmi-best-single-model-simplified/output\n- https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet","metadata":{}},{"cell_type":"markdown","source":"# Notes on Successful approaches:\n- The best performing public model uses Ensemble learning, specifically a **Voting Regressor**, which combines predictions from LightGBM, XGBoost and CatBoost. \"This approach is beneficial as it leverages the strengths of multiple models, reducing overfitting and improving overall model performance.\"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom typing import List\n\nfrom scipy import stats\nimport torch\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import clone\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\n# from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\n# from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix, cohen_kappa_score\nfrom scipy.stats import randint, uniform\nfrom sklearn.feature_selection import mutual_info_classif\nfrom scipy.optimize import minimize\n\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n# from lightgbm import LGBMClassifier\n# from xgboost import XGBClassifier\n# from catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:25:59.245485Z","iopub.execute_input":"2024-12-03T08:25:59.245971Z","iopub.status.idle":"2024-12-03T08:25:59.258662Z","shell.execute_reply.started":"2024-12-03T08:25:59.245932Z","shell.execute_reply":"2024-12-03T08:25:59.257190Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_pred_rounded = np.round(y_pred).clip(0, 3).astype(int)\n    return cohen_kappa_score(y_true, y_pred_rounded, weights='quadratic')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:55:41.882840Z","iopub.execute_input":"2024-12-03T07:55:41.883285Z","iopub.status.idle":"2024-12-03T07:55:41.891220Z","shell.execute_reply.started":"2024-12-03T07:55:41.883250Z","shell.execute_reply":"2024-12-03T07:55:41.889935Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    \n    # Get basic stats for ALL columns except step\n    df.drop('step', axis=1, inplace=True)\n    basic_stats = df.describe().values.reshape(-1)\n    \n    # Add derived features while keeping all original columns\n    df['hour'] = pd.to_datetime(df['time_of_day']).dt.hour\n    df['time_gap'] = pd.to_datetime(df['time_of_day']).diff().dt.total_seconds()\n    \n    # Define inactivity using all available information\n    enmo_threshold = 0.003\n    is_inactive = (\n        (df['non-wear_flag'] == 1) |\n        (df['enmo'] < enmo_threshold) |\n        (df['time_gap'] > 5)\n    )\n    \n    additional_features = [\n        (~is_inactive).mean(),  # Activity ratio\n        df[df['hour'].between(9, 17)]['enmo'].mean() or 0,  # Daytime activity\n        df[~df['hour'].between(9, 17)]['enmo'].mean() or 0,  # Non-daytime activity\n        df['time_gap'].gt(5).mean()  # Proportion of large gaps\n    ]\n    \n    features = np.concatenate([basic_stats, additional_features])\n    return features, filename.split('=')[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:54:08.083933Z","iopub.execute_input":"2024-12-03T08:54:08.084422Z","iopub.status.idle":"2024-12-03T08:54:08.095024Z","shell.execute_reply.started":"2024-12-03T08:54:08.084385Z","shell.execute_reply":"2024-12-03T08:54:08.093360Z"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":"def preprocess_data(df: pd.DataFrame, is_train: bool = False, encoders: dict = None, return_encoders: bool = False) -> pd.DataFrame:\n    \"\"\"Preprocess data including encoding and imputation\"\"\"\n    df = df.copy()\n    \n    # First handle missing values\n    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n    for col in numerical_cols:\n        df[col] = df[col].fillna(df[col].median())\n        \n    # Identify categorical columns (including seasons)\n    categorical_cols = df.select_dtypes(include=['object']).columns\n    \n    # Handle missing categoricals\n    for col in categorical_cols:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    \n    # Encode categorical variables\n    if is_train:\n        # For training data, create new encoders\n        encoders = {}\n        for col in categorical_cols:\n            if col != 'id':  # Skip ID column\n                encoders[col] = LabelEncoder()\n                df[col] = encoders[col].fit_transform(df[col])\n        # Decide whether to return encoders\n        return (df, encoders) if return_encoders else df\n    else:\n        if encoders is None:\n            raise ValueError(\"Must provide encoders when preprocessing test data\")\n        # For test data, use provided encoders\n        for col in categorical_cols:\n            if col != 'id' and col in encoders:  # Skip ID and handle only columns present in train\n                df[col] = encoders[col].transform(df[col])\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:55:42.539968Z","iopub.execute_input":"2024-12-03T07:55:42.540398Z","iopub.status.idle":"2024-12-03T07:55:42.551204Z","shell.execute_reply.started":"2024-12-03T07:55:42.540366Z","shell.execute_reply":"2024-12-03T07:55:42.549767Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def feature_engineering(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:07:59.929780Z","iopub.execute_input":"2024-12-03T08:07:59.930800Z","iopub.status.idle":"2024-12-03T08:07:59.944781Z","shell.execute_reply.started":"2024-12-03T08:07:59.930735Z","shell.execute_reply":"2024-12-03T08:07:59.943271Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def find_correlated_features(X, threshold=0.95):\n    \"\"\"Identifies correlated features to remove based on training data\"\"\"\n    corr_matrix = X.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    # Find features to drop\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print(f\"Identified {len(to_drop)} correlated features to remove\")\n    return to_drop\n\ndef remove_correlated_features(X, features_to_drop):\n    \"\"\"Removes specified features from a dataset\"\"\"\n    X_reduced = X.drop(columns=features_to_drop)\n    return X_reduced","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:55:42.808445Z","iopub.execute_input":"2024-12-03T07:55:42.808902Z","iopub.status.idle":"2024-12-03T07:55:42.816464Z","shell.execute_reply.started":"2024-12-03T07:55:42.808865Z","shell.execute_reply":"2024-12-03T07:55:42.815092Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"class_weights = {\n    0: 1.0,\n    1: 4.0,  \n    2: 8.0,  \n    3: 32.0  \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:40:59.905112Z","iopub.execute_input":"2024-12-03T08:40:59.905637Z","iopub.status.idle":"2024-12-03T08:40:59.912536Z","shell.execute_reply.started":"2024-12-03T08:40:59.905584Z","shell.execute_reply":"2024-12-03T08:40:59.911025Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"def threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:54:32.976754Z","iopub.execute_input":"2024-12-03T08:54:32.977273Z","iopub.status.idle":"2024-12-03T08:54:32.984447Z","shell.execute_reply.started":"2024-12-03T08:54:32.977234Z","shell.execute_reply":"2024-12-03T08:54:32.983295Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"def get_device_params():\n    # if os.environ.get('KAGGLE_IS_KAGGLE_GPU_ENABLED', '') == 'True':\n    if torch.cuda.is_available():\n        XGB_device = {'tree_method': 'gpu_hist'}\n        CB_device = {'task_type': 'GPU'}\n        print(\"GPU detected - using GPU accelerated training\")\n    else:\n        XGB_device = {'tree_method': 'hist'}\n        CB_device = {'task_type': 'CPU'}\n        print(\"No GPU detected - using CPU training\")\n    return XGB_device, CB_device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:55:43.332751Z","iopub.execute_input":"2024-12-03T07:55:43.333686Z","iopub.status.idle":"2024-12-03T07:55:43.340154Z","shell.execute_reply.started":"2024-12-03T07:55:43.333638Z","shell.execute_reply":"2024-12-03T07:55:43.338803Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"xgb_device, cb_device = get_device_params()\n\nSEED = 42\nn_splits = 5\n\nLGB_Params = {\n    'learning_rate': 0.07, \n    'random_state': SEED, \n    'n_estimators': 200,\n    \n    'max_depth': 8, \n    'num_leaves': 300, \n    'min_data_in_leaf': 17,\n    # 'max_depth': 6,  # Reduced from 8\n    # 'min_data_in_leaf': 10,  # Reduced from 17\n    # 'num_leaves': 150,  # Reduced from 300\n    \n    'feature_fraction': 0.7689, \n    'bagging_fraction': 0.6879, \n    'bagging_freq': 2, \n    'lambda_l1': 4.74, \n    'lambda_l2': 4.743e-06,\n    'verbose': -1,\n    'class_weight': class_weights,\n    'min_child_samples': 5\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED,\n    **xgb_device  # This will add either GPU or CPU parameters\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10,\n    **cb_device  # This will add either GPU or CPU parameters\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:35:04.656857Z","iopub.execute_input":"2024-12-03T08:35:04.658119Z","iopub.status.idle":"2024-12-03T08:35:04.670660Z","shell.execute_reply.started":"2024-12-03T08:35:04.658039Z","shell.execute_reply":"2024-12-03T08:35:04.669315Z"}},"outputs":[{"name":"stdout","text":"No GPU detected - using CPU training\n","output_type":"stream"}],"execution_count":124},{"cell_type":"code","source":"models = {\n    'lgbm': LGBMRegressor(**LGB_Params),\n    'xgboost': XGBRegressor(**XGB_Params),\n    'catboost': CatBoostRegressor(**CatBoost_Params)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:35:07.379058Z","iopub.execute_input":"2024-12-03T08:35:07.379482Z","iopub.status.idle":"2024-12-03T08:35:07.385599Z","shell.execute_reply.started":"2024-12-03T08:35:07.379447Z","shell.execute_reply":"2024-12-03T08:35:07.384157Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"def analyze_predictions_distribution(y_true, predictions, name=\"\"):\n    print(f\"\\n{'-'*20} {name} Prediction Analysis {'-'*20}\")\n    \n    # Get distribution of predictions\n    pred_dist = pd.Series(predictions).value_counts(normalize=True).sort_index() * 100\n    \n    if y_true is not None:\n        # Get distribution of true values\n        true_dist = pd.Series(y_true).value_counts(normalize=True).sort_index() * 100\n        \n        print(\"\\nClass Distribution Comparison:\")\n        comparison_df = pd.DataFrame({\n            'True %': true_dist,\n            'Predicted %': pred_dist\n        }).round(2)\n        print(comparison_df)\n        \n        # Confusion matrix for detailed analysis\n        conf_matrix = pd.crosstab(y_true, predictions, \n                                normalize=True) * 100\n        print(\"\\nConfusion Matrix (% of true classes predicted as each class):\")\n        print(conf_matrix.round(2))\n        \n        # Analysis of severe cases (class 3)\n        if 3 in y_true:\n            severe_mask = y_true == 3\n            severe_predictions = predictions[severe_mask]\n            print(f\"\\nDetailed Analysis of Severe Cases (Class 3):\")\n            print(f\"Total Severe Cases: {sum(severe_mask)}\")\n            print(\"Predicted as:\")\n            for pred_class in range(4):\n                count = sum(severe_predictions == pred_class)\n                percent = (count / sum(severe_mask)) * 100\n                print(f\"Class {pred_class}: {count} cases ({percent:.2f}%)\")\n    else:\n        print(\"\\nPrediction Distribution:\")\n        for class_val, percentage in pred_dist.items():\n            print(f\"Class {class_val}: {percentage:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:55:25.850542Z","iopub.execute_input":"2024-12-03T08:55:25.851061Z","iopub.status.idle":"2024-12-03T08:55:25.862285Z","shell.execute_reply.started":"2024-12-03T08:55:25.851021Z","shell.execute_reply":"2024-12-03T08:55:25.860741Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"# WITH SMOTE\ndef TrainML(model_class, test_data):\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        # print(\"Training fold class distribution:\")\n        # print(y_train.value_counts(normalize=True).sort_index() * 100)\n        # print(\"\\nRaw counts:\")\n        # print(y_train.value_counts().sort_index())\n\n        # smote = SMOTE(random_state=SEED) # basic smote\n        \n        # aggresive smote (matching distribution more closely)\n        class_counts = y_train.value_counts().sort_index()\n        majority_count = class_counts[0]  # count of class 0\n        \n        smote = SMOTE(\n            random_state=SEED,\n            sampling_strategy={\n                1: int(majority_count * 0.3),  # 30% of majority class\n                2: int(majority_count * 0.2),  # 20% of majority class\n                3: int(majority_count * 0.1)   # 10% of majority class\n            }\n        )\n\n\n                      \n        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n        X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train.columns)\n        \n        model = clone(model_class)\n        # model.fit(X_train, y_train)\n        model.fit(X_train_resampled, y_train_resampled)\n\n        # y_train_pred = model.predict(X_train)\n        # y_val_pred = model.predict(X_val)\n        y_train_pred = np.clip(model.predict(X_train), 0, 3)\n        y_val_pred = np.clip(model.predict(X_val), 0, 3)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        # test_preds[:, fold] = model.predict(test_data)\n        test_preds[:, fold] = np.clip(model.predict(test_data), 0, 3)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    analyze_predictions_distribution(y, oof_rounded, \"OOF\")\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              # x0=[0.3, 1.0, 2.0], args=(y, oof_non_rounded), # to increase prob of predicting class 3\n                              method='Nelder-Mead') # Nelder-Mead | # Powell\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n\n    # analyze_predictions_distribution(None, tpTuned, \"Ensemble Test\")\n    \n    submission = pd.DataFrame({\n        'id': test_id,     #sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:55:26.373596Z","iopub.execute_input":"2024-12-03T08:55:26.374085Z","iopub.status.idle":"2024-12-03T08:55:26.390237Z","shell.execute_reply.started":"2024-12-03T08:55:26.374047Z","shell.execute_reply":"2024-12-03T08:55:26.388624Z"}},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":"# Steps for improvement:\n- Address class imbalance\n    - Modify your StratifiedKFold implementation to ensure the rare class (severity 3) is properly represented in each fold:\n    - Add sample weights to give more importance to minority classes:\n    - Modify your threshold optimization to be more sensitive to minority classes:\n    - Adjust model parameters for imbalanced data:\n\n- Add more new features (look at what other contestants are doing)\n- Modify StatifiedKFold to use group-based splitting \n- Consider making model weights adaptive based on validation performance\n- Make threshold rounder more robust\n- Correlation threshold of 0.95 might be too high","metadata":{}},{"cell_type":"code","source":"# Load data\nprint(\"Loading data...\")\nbase_path = '/kaggle/input/child-mind-institute-problematic-internet-use'\ntrain_parquet_path = f'{base_path}/series_train.parquet'\ntest_parquet_path = f'{base_path}/series_test.parquet'\n\n# Load data\ntrain = pd.read_csv(f'{base_path}/train.csv')\ntest = pd.read_csv(f'{base_path}/test.csv')\n\n# Get supervised data (rows with non-null target)\n# train = train[train['sii'].notna()].copy()\nprint(f\"Train shape: {train.shape}, Test shape: {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.243427Z","iopub.execute_input":"2024-12-03T08:17:10.243887Z","iopub.status.idle":"2024-12-03T08:17:10.291982Z","shell.execute_reply.started":"2024-12-03T08:17:10.243851Z","shell.execute_reply":"2024-12-03T08:17:10.290651Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nTrain shape: (3960, 82), Test shape: (20, 59)\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"# # !ls\n# print(os.getcwd())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.395200Z","iopub.execute_input":"2024-12-03T08:17:10.396080Z","iopub.status.idle":"2024-12-03T08:17:10.402827Z","shell.execute_reply.started":"2024-12-03T08:17:10.396039Z","shell.execute_reply":"2024-12-03T08:17:10.401707Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# Load parquet features with error handling\nprint(\"\\nProcessing parquet files...\")\ntry:\n    print(\"Loading and processing training data...\")\n    train_ts = load_time_series(train_parquet_path)\n    print(\"\\nShape of training features:\", train_ts.shape)\n    \n    print(\"\\nLoading and processing test data...\")\n    test_ts = load_time_series(test_parquet_path)\n    print(\"\\nShape of test features:\", test_ts.shape)\n    \n    # Save features\n    print(\"\\nSaving features...\")\n    train_ts.to_parquet(\"actigraphy_features_train.parquet\")\n    test_ts.to_parquet(\"actigraphy_features_test.parquet\")\nexcept Exception as e:\n    print(f\"Error processing parquet files: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.553525Z","iopub.execute_input":"2024-12-03T08:17:10.553992Z","iopub.status.idle":"2024-12-03T08:17:10.559976Z","shell.execute_reply.started":"2024-12-03T08:17:10.553955Z","shell.execute_reply":"2024-12-03T08:17:10.558613Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"print(\"\\nLoading parquet data...\")\ntry:\n    train_ts = pd.read_parquet(\"actigraphy_features_train.parquet\")\n    test_ts = pd.read_parquet(\"actigraphy_features_test.parquet\")\nexcept Exception as e:\n    print(f\"Error in merging: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.687864Z","iopub.execute_input":"2024-12-03T08:17:10.688343Z","iopub.status.idle":"2024-12-03T08:17:10.719112Z","shell.execute_reply.started":"2024-12-03T08:17:10.688304Z","shell.execute_reply":"2024-12-03T08:17:10.717794Z"}},"outputs":[{"name":"stdout","text":"\nLoading parquet data...\n","output_type":"stream"}],"execution_count":102},{"cell_type":"code","source":"# Merge with original data\nprint(\"\\nMerging parquet data with CSV data...\")\ntry:\n    train = pd.merge(train, train_ts, how=\"left\", on='id')\n    test = pd.merge(test, test_ts, how=\"left\", on='id')\n\n    print(\"\\nInitial shapes:\")\n    print(\"Train combined:\", train.shape)\n    print(\"Test combined:\", test.shape)\nexcept Exception as e:\n    print(f\"Error in merging: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.821740Z","iopub.execute_input":"2024-12-03T08:17:10.822154Z","iopub.status.idle":"2024-12-03T08:17:10.843451Z","shell.execute_reply.started":"2024-12-03T08:17:10.822123Z","shell.execute_reply":"2024-12-03T08:17:10.842167Z"}},"outputs":[{"name":"stdout","text":"\nMerging parquet data with CSV data...\n\nInitial shapes:\nTrain combined: (3960, 182)\nTest combined: (20, 159)\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"# Impute using KNN (need to undo line where I remove missing target rows)\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:10.955815Z","iopub.execute_input":"2024-12-03T08:17:10.956231Z","iopub.status.idle":"2024-12-03T08:17:29.860000Z","shell.execute_reply.started":"2024-12-03T08:17:10.956201Z","shell.execute_reply":"2024-12-03T08:17:29.858923Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# Preprocess with error handling\nprint(\"\\nPreprocessing data...\")\ntry:\n    train, encoders = preprocess_data(train, is_train=True, return_encoders=True)\n    test = preprocess_data(test, is_train=False, encoders=encoders)\nexcept Exception as e:\n    print(f\"Error in preprocessing: {str(e)}\")\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:29.861810Z","iopub.execute_input":"2024-12-03T08:17:29.862122Z","iopub.status.idle":"2024-12-03T08:17:30.073634Z","shell.execute_reply.started":"2024-12-03T08:17:29.862093Z","shell.execute_reply":"2024-12-03T08:17:30.072468Z"}},"outputs":[{"name":"stdout","text":"\nPreprocessing data...\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"train = feature_engineering(train)\ntrain = train.dropna(thresh=10, axis=0) # only keep rows with at least 10 Non-null\ntest = feature_engineering(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:30.075075Z","iopub.execute_input":"2024-12-03T08:17:30.075385Z","iopub.status.idle":"2024-12-03T08:17:30.128905Z","shell.execute_reply.started":"2024-12-03T08:17:30.075357Z","shell.execute_reply":"2024-12-03T08:17:30.127802Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"print(\"\\nLast Prep for Training...\")\ntry:\n    exclude_patterns = ['id', 'PCIAT', 'sii']\n    feature_cols = [col for col in train.columns \n                   if not any(pattern in col for pattern in exclude_patterns)]\n    X = train[feature_cols]\n    y = train['sii']\n    features_to_drop = find_correlated_features(X, threshold=0.95) \n    X = remove_correlated_features(X, features_to_drop)\n    \n    test_id = test['id'].copy()\n    test = test[feature_cols]\n    test = remove_correlated_features(test, features_to_drop)\nexcept Exception as e:\n    print()\n    raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:17:30.131841Z","iopub.execute_input":"2024-12-03T08:17:30.132315Z","iopub.status.idle":"2024-12-03T08:17:30.455801Z","shell.execute_reply.started":"2024-12-03T08:17:30.132266Z","shell.execute_reply":"2024-12-03T08:17:30.454590Z"}},"outputs":[{"name":"stdout","text":"\nLast Prep for Training...\nIdentified 47 correlated features to remove\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"# Train models\nprint(\"\\nTraining models...\")\ntry:\n    models = LGBMRegressor(**LGB_Params)\n    # models = {\n        # 'lgbm': LGBMRegressor(**LGB_Params),\n        # 'xgboost': XGBRegressor(**XGB_Params),\n        # 'catboost': CatBoostRegressor(**CatBoost_Params)\n    # }\n    # ensemble_submission, scores, weights = weighted_ensemble_train(X, y, test_features, models)\n    submission, trained_model = TrainML(models, test)\nexcept Exception as e:\n    print(f\"Error in model training: {str(e)}\")\n    raise\n\n# Save submission\nprint(\"\\nSaving submission...\")\n# ensemble_submission.to_csv('submission.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:52:33.493698Z","iopub.execute_input":"2024-12-03T08:52:33.494908Z","iopub.status.idle":"2024-12-03T08:52:43.830479Z","shell.execute_reply.started":"2024-12-03T08:52:33.494864Z","shell.execute_reply":"2024-12-03T08:52:43.829213Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:09<00:00,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"\n-------------------- OOF Prediction Analysis --------------------\n\nClass Distribution Comparison:\n     True %  Predicted %\n0.0   71.16        54.49\n1.0   18.43        40.73\n2.0    9.55         4.65\n3.0    0.86         0.13\n\nConfusion Matrix (% of true classes predicted as each class):\ncol_0      0      1     2     3\nsii                            \n0.0    48.66  21.34  1.16  0.00\n1.0     4.52  12.30  1.62  0.00\n2.0     1.31   6.59  1.52  0.13\n3.0     0.00   0.51  0.35  0.00\n\nDetailed Analysis of Severe Cases (Class 3):\nTotal Severe Cases: 34\nPredicted as:\nClass 0: 0 cases (0.00%)\nClass 1: 20 cases (58.82%)\nClass 2: 14 cases (41.18%)\nClass 3: 0 cases (0.00%)\nMean Train QWK --> 0.8494\nMean Validation QWK ---> 0.4491\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.481\u001b[0m\n\nSaving submission...\nDone!\n","output_type":"stream"}],"execution_count":150}]}